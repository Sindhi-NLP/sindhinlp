import unittest
import pandas as pd
from sindhinlp.preprocess.tokenize import tokenize_words,tokenize_sentences, costum_tokenize
from sindhinlp.preprocess.stopwords import get_stopwords, get_extend_stopwords
from sindhinlp.preprocess.text_preprocessing import remove_special_characters, remove_stopwords, remove_urls, remove_emoji,remove_number
from sindhinlp.preprocess.lematize import lemmatize
from sindhinlp.models.pos_tagging import pos_tags
from sindhinlp.data.load_data import load_dataset
from typing import FrozenSet

class TestTokenization(unittest.TestCase):

    def test_tokenize_words(self):
        text = "Ø³Ù†ÚŒÙŠ Ø²Ø¨Ø§Ù† Ø¬Ùˆ Ù…ÙˆÙ†Ú¾Ù†Ø¬Ùˆ Ø¹Ù„Ù…ØŒ ØªØ§Ø±ÙŠØ®."
        expected_tokens = ['Ø³Ù†ÚŒÙŠ', 'Ø²Ø¨Ø§Ù†', 'Ø¬Ùˆ', 'Ù…ÙˆÙ†Ú¾Ù†Ø¬Ùˆ', 'Ø¹Ù„Ù…', 'ØªØ§Ø±ÙŠØ®']
        tokens = tokenize_words(text)
        self.assertEqual(tokens, expected_tokens)

    def test_tokenize_sentences(self):
        text = "Ø³Ù†ÚŒÙŠ Ø²Ø¨Ø§Ù† Ø¬Ùˆ Ù…ÙˆÙ†Ú¾Ù†Ø¬Ùˆ Ø¹Ù„Ù…ØŒ ØªØ§Ø±ÙŠØ® Û½ Ø«Ù‚Ø§ÙØª Û¾ Ø³Ù†ÚŒ Ø¬ÙŠ ÚŠÙˆÚªØ±ÙŠÙˆÙ†ØŒ Ù„ÙˆÚªÙŠÙˆÙ† Û½ ÚªØªØ§Ø¨ÙˆÙ† Û¾ Ø³Ù…Ø¬Ù‡Ø§ÙŠÙˆ Ø¬ÙŠÙˆÙ† ÚªØ±ØªÙˆØªÛ” Ø³Ù†ÚŒÙŠ Ø¬ÙŠ Ø¹ÙˆØ§Ù… Ú©ÙŠ Ø³Ù…Ø¬Ù‡Ø§Ú» Ø¬Ùˆ Ø±ÙˆÙŠØ§ Ù¾Ù†Ù‡Ù†Ø¬ÙŠ Ø²Ø¨Ø§Ù† Û¾ Ø³Ø§Ø¦Ù†Ø³ØŒ ÙÙ†ÙˆÙ† Û½ Ø³Ø§ÙŠÙ†Ø³ ÚªØ§Ù† Ù…ØªØ¹Ù„Ù‚ Ø§Ø­Ø§Ø·Ùˆ Ø³ÙˆØ§Ù†Ø­ Û½ ÚªØ±ØªÙˆØª Û½ Ù…Ø­Ø§ÙˆØ±Ù† Ø¬Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯ÙŠÙ†Ø¯Ùˆ Ø¢Ù‡ÙŠ."
        expected_sentences = [
            'Ø³Ù†ÚŒÙŠ Ø²Ø¨Ø§Ù† Ø¬Ùˆ Ù…ÙˆÙ†Ú¾Ù†Ø¬Ùˆ Ø¹Ù„Ù…ØŒ ØªØ§Ø±ÙŠØ® Û½ Ø«Ù‚Ø§ÙØª Û¾ Ø³Ù†ÚŒ Ø¬ÙŠ ÚŠÙˆÚªØ±ÙŠÙˆÙ†ØŒ Ù„ÙˆÚªÙŠÙˆÙ† Û½ ÚªØªØ§Ø¨ÙˆÙ† Û¾ Ø³Ù…Ø¬Ù‡Ø§ÙŠÙˆ Ø¬ÙŠÙˆÙ† ÚªØ±ØªÙˆØªÛ”',
            'Ø³Ù†ÚŒÙŠ Ø¬ÙŠ Ø¹ÙˆØ§Ù… Ú©ÙŠ Ø³Ù…Ø¬Ù‡Ø§Ú» Ø¬Ùˆ Ø±ÙˆÙŠØ§ Ù¾Ù†Ù‡Ù†Ø¬ÙŠ Ø²Ø¨Ø§Ù† Û¾ Ø³Ø§Ø¦Ù†Ø³ØŒ ÙÙ†ÙˆÙ† Û½ Ø³Ø§ÙŠÙ†Ø³ ÚªØ§Ù† Ù…ØªØ¹Ù„Ù‚ Ø§Ø­Ø§Ø·Ùˆ Ø³ÙˆØ§Ù†Ø­ Û½ ÚªØ±ØªÙˆØª Û½ Ù…Ø­Ø§ÙˆØ±Ù† Ø¬Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯ÙŠÙ†Ø¯Ùˆ Ø¢Ù‡ÙŠ.'
        ]
        sentences = tokenize_sentences(text)
        self.assertEqual(sentences, expected_sentences)

    def test_custom_tokenize(self):
        text = "Ø³Ù†ÚŒÙŠ Ø²Ø¨Ø§Ù†ØŒ Ø¬ÙŠ Ù…ÙˆÙ†Ú¾Ù†Ø¬Ùˆ Ø¹Ù„Ù…ØŒ ØªØ§Ø±ÙŠØ® Û½ Ø«Ù‚Ø§ÙØª"        
        expected_sentences = ['Ø³Ù†ÚŒÙŠ Ø²Ø¨Ø§Ù†', ' Ø¬ÙŠ Ù…ÙˆÙ†Ú¾Ù†Ø¬Ùˆ Ø¹Ù„Ù…', ' ØªØ§Ø±ÙŠØ® Û½ Ø«Ù‚Ø§ÙØª']
        sentences = costum_tokenize(text,"ØŒ")
        self.assertEqual(sentences , expected_sentences)

    def test_custom_tokenize(self):
        text = "Ø³Ù†ÚŒÙŠ Ø²Ø¨Ø§Ù†ØŒ Ø¬ÙŠ Ù…ÙˆÙ†Ú¾Ù†Ø¬Ùˆ Ø¹Ù„Ù…ØŒ ØªØ§Ø±ÙŠØ® Û½ Ø«Ù‚Ø§ÙØª"        
        expected_sentences = ['Ø³Ù†ÚŒÙŠ Ø²Ø¨Ø§Ù†', ' Ø¬ÙŠ Ù…ÙˆÙ†Ú¾Ù†Ø¬Ùˆ Ø¹Ù„Ù…', ' ØªØ§Ø±ÙŠØ® Û½ Ø«Ù‚Ø§ÙØª']
        sentences = costum_tokenize(text,"ØŒ")
        self.assertEqual(sentences , expected_sentences)

    def test_get_stopwords(self):
        stopwords = get_stopwords()
        assert isinstance(stopwords, FrozenSet)
        assert len(stopwords) == 606
        assert 'Ù‡ÚØ§' in stopwords
        assert 'Ú‡Ø§' in stopwords 

    def test_get_extend_stopwords(self):
        
        extended_set = {'Ù†Ø¦ÙˆÙ†', 'Ø§Ø¶Ø§ÙÙŠ'}
        extended_stopwords = get_extend_stopwords(extended_set)
        assert isinstance(extended_stopwords, FrozenSet)
        assert len(extended_stopwords) == 608  # 606 from original + 2 new
        assert 'Ù‡ÚØ§' in extended_stopwords
        assert 'Ú‡Ø§' in extended_stopwords 
        assert 'Ù†Ø¦ÙˆÙ†' in extended_stopwords
        assert 'Ø§Ø¶Ø§ÙÙŠ' in extended_stopwords

    def test_get_extend_stopwords_empty_set(self):
        extended_stopwords = get_extend_stopwords(set())
        assert isinstance(extended_stopwords, FrozenSet)
        assert len(extended_stopwords) == 606 # Same as original set
        assert extended_stopwords == get_stopwords()

    def test_get_extend_stopwords_duplicate(self):
        extended_set = {'Ù‡ÚØ§', 'Ù†Ø¦ÙˆÙ†'}  # 'Ù‡ÚØ§' is already in the original set
        extended_stopwords = get_extend_stopwords(extended_set)
        assert len(extended_stopwords) == 607  # 606 from original + 1 new
        assert isinstance(extended_stopwords, FrozenSet)

    def test_remove_special_characters(self):
        text = "Ø³Ù†ÚŒÙŠ Ø²Ø¨Ø§Ù† Ø¬Ùˆ Ù…ÙˆÙ†Ú¾Ù†Ø¬Ùˆ Ø¹Ù„Ù…ØŒ ØªØ§Ø±ÙŠØ® Û½ Ø«Ù‚Ø§ÙØªÛ” ÚªØªØ§Ø¨ Ø¬Ùˆ Ø¹Ù†ÙˆØ§Ù†: 'Ø³Ù†ÚŒ Ø¬Ùˆ Ø§Ù“Ø³Ù…Ø§Ù†'"
        expected_text = "Ø³Ù†ÚŒÙŠ Ø²Ø¨Ø§Ù† Ø¬Ùˆ Ù…ÙˆÙ†Ú¾Ù†Ø¬Ùˆ Ø¹Ù„Ù… ØªØ§Ø±ÙŠØ® Û½ Ø«Ù‚Ø§ÙØª ÚªØªØ§Ø¨ Ø¬Ùˆ Ø¹Ù†ÙˆØ§Ù† Ø³Ù†ÚŒ Ø¬Ùˆ Ø§Ù“Ø³Ù…Ø§Ù†"
        clean_text = remove_special_characters(text)
        self.assertEqual(clean_text, expected_text)

    def test_remove_stopwords(self):
        text = "Ù…Ø­Ø§ÙˆØ±Ù† Ø¬Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯ÙŠÙ†Ø¯Ùˆ Ø¢Ù‡ÙŠ"
        expected_text = "Ù…Ø­Ø§ÙˆØ±Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯ÙŠÙ†Ø¯Ùˆ"
        stopword_removed_text = remove_stopwords(text)
        self.assertEqual(stopword_removed_text , expected_text)

    def test_remove_number(self):
        sindhi_text_with_numbers = "Ù‡ÙŠ 1234 Ù½ÙŠØ³Ù½ 5678 Ù…ÙˆØ§Ø¯ Ø¢Ù‡ÙŠ 90 Ù†Ù…Ø¨Ø± Ø¬ÙŠ ÙˆÚ† Û¾."
        expected_sindhi_result_no_numbers = "Ù‡ÙŠ Ù½ÙŠØ³Ù½ Ù…ÙˆØ§Ø¯ Ø¢Ù‡ÙŠ Ù†Ù…Ø¨Ø± Ø¬ÙŠ ÙˆÚ† Û¾."
        result_sindhi_no_numbers = remove_number(sindhi_text_with_numbers)
        assert result_sindhi_no_numbers == expected_sindhi_result_no_numbers, f"Expected: '{expected_sindhi_result_no_numbers}', but got: '{result_sindhi_no_numbers}'"

    def test_remove_urls(self):
        sindhi_text_with_urls = "Ú¾ÙŠ Ù„Ù†Úª Ú†ÙŠÚª ÚªØ±ÙŠÙˆ: http://example.com Û½ Ù¾Ú» ÙˆØ²Ù½ ÚªØ±ÙŠÙˆ https://www.example.org ÙˆÚŒÙŠÚª Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„Ø§Ø¡Ù."
        expected_sindhi_result_no_urls = "Ú¾ÙŠ Ù„Ù†Úª Ú†ÙŠÚª ÚªØ±ÙŠÙˆ: Û½ Ù¾Ú» ÙˆØ²Ù½ ÚªØ±ÙŠÙˆ ÙˆÚŒÙŠÚª Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„Ø§Ø¡Ù."
        result_sindhi_no_urls = remove_urls(sindhi_text_with_urls)
        assert result_sindhi_no_urls == expected_sindhi_result_no_urls, f"Expected: '{expected_sindhi_result_no_urls}', but got: '{result_sindhi_no_urls}'"
    
    def test_remove_emoji(self):
        sindhi_text_with_emojis = "Ú¾ÙŠ Ù½ÙŠØ³Ù½ Ù…ÙˆØ§Ø¯ Ø¢Ú¾ÙŠ ğŸ˜ŠğŸš€ğŸŒŸ Ø¬ÙŠ ÙˆÚ† Û¾."
        expected_sindhi_result_no_emojis = "Ú¾ÙŠ Ù½ÙŠØ³Ù½ Ù…ÙˆØ§Ø¯ Ø¢Ú¾ÙŠ Ø¬ÙŠ ÙˆÚ† Û¾."
        result_sindhi_no_emojis = remove_emoji(sindhi_text_with_emojis)
        assert result_sindhi_no_emojis == expected_sindhi_result_no_emojis, f"Expected: '{expected_sindhi_result_no_emojis}', but got: '{result_sindhi_no_emojis}'"

    def test_pos_tagger(self):
        sentence = ". Ù…ÙˆØ³Ù… Ø¨Ù‡ØªØ±ÙŠÙ† Ø¢Ù‡ÙŠ."
        tags = pos_tags(sentence)
        assert tags == ['PERIOD', 'NOUN', 'ADJ', 'NOUN']

    def test_lemmatize(self):
        word = "Ø±Ù‡Ù†Ø¯Ùˆ"
        lemma = 'Ø±Ù‡'
        assert lemmatize(word) == lemma

    def test_load_dataset_default(self):
        df = load_dataset()
        assert isinstance(df, pd.DataFrame)
        assert list(df.columns) == ['Unnamed: 0', 'article', 'link', 'title', 'genre']

    def test_load_dataset_article_dataset(mock_csv):
        df = load_dataset('article_dataset')
        assert isinstance(df, pd.DataFrame)
        assert list(df.columns) == ['Unnamed: 0', 'article', 'link', 'date', 'author']


if __name__ == '__main__':
    unittest.main()